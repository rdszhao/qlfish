{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSCI 551: final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](211.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qlfish: database query system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overview\n",
    "`qlfish` was designed for efficiency and user-friendliness while remaining robust for complex queries. It integrates advanced features such as data ingestion, complex query processing, and data streaming.\n",
    "\n",
    "### objectives\n",
    "1. **efficient data handling**: manages large volumes of data, ensuring quick access and manipulation, including the ability to ingest data from various sources, store it effectively, and retrieve it promptly when needed\n",
    "\n",
    "2. **complex query processing**: executes a range of queries, from basic crud (create, read, update, delete) operations to more complex ones involving aggregation, joins, and group-by functionalities. the system is designed to handle these efficiently and accurately\n",
    "\n",
    "3. **user-friendly interface**: provides an intuitive command interface that simplifies the process of executing queries and managing data, with a familiar `sql`-inspired syntax\n",
    "\n",
    "in the following sections, we delve into the technical architecture, core functionalities, and the technical principles underpinning this database query system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overview\n",
    "the architecture comprises several interconnected components, each designed to handle specific aspects of data management and query processing\n",
    "\n",
    "### components\n",
    "\n",
    "#### `database` class\n",
    "\n",
    "- **role and functionality**: the `database` class is the backbone of the system. it manages the storage, retrieval, and manipulation of data. this class is initialized with a memory limit, which determines the chunk sizes for data to ensure nothing runs out of core\n",
    "- **key methods**:\n",
    "  - **`ingest`**: readies `csv`s for import into the system and calculating the chunk sizing\n",
    "  - **`open_stream`**: creates a data stream from a specified table, enabling streamed data handling\n",
    "  - **`create_table`**: creates new tables within the database for storing data.\n",
    "  - **`insert`**, **`update`**, **`delete`**: these methods facilitate the basic crud operations on the data stored in the tables\n",
    "  - **`query`**: endpoint for all query execution (select, join, aggregate, etc.) on the data\n",
    "\n",
    "#### `query` class\n",
    "- **purpose**: the `query` class is designed to handle the execution and management of different types of queries in the form of nested subqueries\n",
    "- **functionality**: it interprets query parameters and calls appropriate methods in the `database` class to fetch or manipulate data. this class is critical for performing complex operations like joins, aggregations, and group-by\n",
    "- **query processing**: it includes processing conditions, aggregating results, and managing the flow of data for join operations\n",
    "\n",
    "#### command interface\n",
    "- **function**: the `commandinterface` class serves as the user interface for interacting with the database. it interprets user inputs, converts them into commands or queries, and communicates with the `database` and `query` classes to execute these operations\n",
    "- **user interaction**: through a simple command-line interface, users can input their queries, which are then parsed and executed by the system\n",
    "\n",
    "### data flow and interaction\n",
    "- **data ingestion and storage**: data is ingested through the `database` class and stored in a structured format within the system, streamed from when called upon\n",
    "- **query execution**: when a query is input, the `commandinterface` parses it and interacts with the `query` class to process it, which then calls `open_stream` on the required data\n",
    "- **result retrieval**: after processing, the results are sent back to the `commandinterface`, which displays them to the user\n",
    "\n",
    "### scalability and performance optimization\n",
    "- **memory management**: the system is designed to handle large datasets efficiently, utilizing streaming and chunking to optimize memory usage\n",
    "- **indexing and caching strategies**: these are implemented for faster data joining and query execution\n",
    "\n",
    "### robustness and error handling\n",
    "- **error detection and handling**: the system includes mechanisms to detect and handle errors, providing feedback for any incorrect or invalid queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## core functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data ingestion\n",
    "- **process overview**: data ingestion is the first step in populating the database with data. the system focuses on `.csv` files\n",
    "- **implementation**: the `ingest` and `open_stream` methods in `database` handle this. `ingest` computes the average chunk size for system-wide use and stores reference to the table for use by `open_stream` when called on to enable data streaming\n",
    "- **chunk-based processing**: to efficiently manage large datasets, the system uses a chunk-based approach for specific types of aggregation while it uses streaming for others.\n",
    "\n",
    "### query processing\n",
    "#### select queries\n",
    "- **methodology**: the `select` method in the `query` class is responsible for executing select queries. it involves filtering and projecting data based on user-defined criteria.\n",
    "- **field specification**: users specify fields or use `*` for all fields. the method constructs a data generator that iterates over the dataset, yielding only the specified fields.\n",
    "- **conditional filtering**: implemented via the `where` function in the `query` class. this function evaluates conditions against each data row. conditions are parsed into logical expressions that compare field values against user-defined criteria.\n",
    "- **sorting**: achieved through the `sort_large_data` method. the method sorts data in memory-efficient chunks, using pythonâ€™s built-in sorting functions. if specified, it uses a temporary file system for large datasets to avoid memory overload.\n",
    "- **chunk-based processing**: for large datasets, data is processed in chunks determined by the `chunk_size` attribute in the `database` class. this approach reduces memory consumption and allows the system to handle larger datasets efficiently.\n",
    "\n",
    "#### aggregation queries\n",
    "- **function implementation**: the `agg` method in the `query` class handles aggregation queries. it supports operations like average (avg), sum, count, min, and max.\n",
    "- **chunk-based aggregation**: data is processed in chunks. for each chunk, aggregation functions are applied to the specified field. the results from each chunk are then combined to produce the final result.\n",
    "- **aggregation logic**: the system employs python's built-in functions (like `sum`, `min`, `max`) and numpy functions for efficient computation. custom logic is used for average and count to handle these operations across chunks.\n",
    "- **group handling**: when used with `group_by`, the aggregation function is applied to each group separately. this is managed by iterating over the groups generated by the `group_by` method and applying the aggregation function to each group.\n",
    "\n",
    "#### join operations\n",
    "- **join logic**: the `join` function in the `query` class manages join operations. it creates a generator that yields rows from multiple tables based on the join condition.\n",
    "- **join types**: currently focused on inner joins, where rows from different tables are combined based on matching values in specified fields.\n",
    "- **data merging**: the function reads rows from each table and compares them based on the join condition. when a match is found, it combines the rows and yields the result.\n",
    "- **handling large tables**: for large tables, the system uses a nested loop join approach. this method involves iterating over one table and for each row, searching for matching rows in the other table.\n",
    "\n",
    "#### group by functionality\n",
    "- **implementation**: the `group_by` method in the `query` class handles the grouping of data.\n",
    "- **grouping mechanism**: the method iterates over the dataset and groups data based on the specified field. it maintains an index mapping group keys to data rows.\n",
    "- **data streaming for groups**: each group is processed as a separate data stream. when combined with aggregation, the system applies the aggregation function to each group's data stream.\n",
    "- **efficiency considerations**: for large datasets, the system processes each group individually to minimize memory usage. it uses a lazy evaluation approach, where data for a group is only processed when required.\n",
    "\n",
    "### crud operations\n",
    "- **create (create table)**:\n",
    "    - **usage**: this operation involves creating new tables in the database, instantiating a new `.csv` file.\n",
    "    - **process**: the `create_table` allows users to define new tables for storing data by creating a new `.csv` file and storing a reference to it for further inserting\n",
    "\n",
    "- **read (query data)**:\n",
    "    - **description**: reading data involves querying the database to retrieve specific information.\n",
    "    - **functionality**: the `select` function underlies this operation, equipped to handle complex queries including conditions and joins.\n",
    "\n",
    "- **update**:\n",
    "    - **purpose**: this operation is used to modify existing data in the database.\n",
    "    - **implementation**: the `update` method allows users to make changes to data entries based on a specified key\n",
    "\n",
    "- **delete**:\n",
    "    - **function**: this operation is used to remove data from the database.\n",
    "    - **method**: the `delete` function handles this, enabling users to specify the deletion key\n",
    "\n",
    "### parsing and lexical analysis\n",
    "- **role of lexical analysis**: lexical analysis, or lexing, is the first step in interpreting user input. it involves breaking down the raw input (a string of characters) into a series of tokens. these tokens represent meaningful units like keywords, operators, identifiers, and literals\n",
    "- **implementation with `ply.lex`**: we use the `ply.lex` module for lexical analysis. this module scans the input string and matches it against predefined patterns (regular expressions) to identify tokens\n",
    "- **token definition**: tokens in our system include keywords like `select`, `from`, `join`, data types, operators (like `==`, `>`, `<`), and other syntax elements. each token type is defined with a regular expression that `ply.lex` uses to recognize and categorize input strings\n",
    "\n",
    "#### syntax parsing with `ply.yacc`\n",
    "- **implementation with `ply.yacc`**: we employ the `ply.yacc` module for syntax parsing. this module takes the stream of tokens produced by the lexer and constructs a parse tree based on the grammar rules we have defined. the parse tree represents the syntactic structure of the input\n",
    "- **grammar rules definition**: the grammar rules specify how tokens can be combined to form valid statements and expressions. for example, a `select` statement must be followed by a list of fields, the keyword `from`, and a table name\n",
    "\n",
    "#### handling complex queries\n",
    "- **nested queries and sub-queries**: our system is capable of handling complex queries, including nested queries and sub-queries. the parsing logic is designed to recognize and correctly interpret these structures, ensuring accurate query execution.\n",
    "\n",
    "#### integration with query processing\n",
    "- **from parsing to execution**: once the input is successfully parsed, the resulting parse tree is used to generate a query object. this object is then passed to the query processing component of the system, which executes the query against the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## workflow + examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the base files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from db import Database \n",
    "from parserfile import get_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrating the ingestion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833\n",
      "1136\n",
      "1136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ingested data/enrollments.csv into enrollments'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Database(100000)\n",
    "parser = get_parser\n",
    "\n",
    "db.ingest('data/students.csv', 'students')\n",
    "db.ingest('data/courses.csv', 'courses')\n",
    "db.ingest('data/enrollments.csv', 'enrollments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrating insert, update, and deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'student_id': 45006, 'student_name': 'James Wu', 'age': 20, 'major': 'Data Science', 'gpa': 3.86} inserted into students\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entry = '{\"student_id\": 45006, \"student_name\": \"James Wu\", \"age\": 20, \"major\": \"Data Science\", \"gpa\": 3.86}'\n",
    "entry_json = json.loads(entry)\n",
    "db.insert('students', entry_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'student_id': 45006, 'student_name': 'James Woo', 'age': 20, 'major': 'Data Science', 'gpa': 3.86} updated in 'students'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entry = '{\"student_id\": 45006, \"student_name\": \"James Woo\", \"age\": 20, \"major\": \"Data Science\", \"gpa\": 3.86}'\n",
    "entry_json = json.loads(entry)\n",
    "db.update('students', on='student_id', document=entry_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'student_id': 45006, 'student_name': 'James Woo', 'age': 20, 'major': 'Data Science', 'gpa': 3.86} deleted from 'students'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entry = '{\"student_id\": 45006, \"student_name\": \"James Woo\", \"age\": 20, \"major\": \"Data Science\", \"gpa\": 3.86}'\n",
    "entry_json = json.loads(entry)\n",
    "db.delete('students', document=entry_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrate the query parsing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(parsed):\n",
    "\ttry:\n",
    "\t\tif 'subq' in parsed:\n",
    "\t\t\tsubq = parsed['subq']\n",
    "\t\t\tif type(subq) == str:\n",
    "\t\t\t\tparsed['tables'] = [subq]\n",
    "\t\t\t\tdel parsed['subq']\n",
    "\t\t\telse:\n",
    "\t\t\t\tparsed['subq'] = generate_query(parsed['subq'])\n",
    "\t\treturn db.query(**parsed)\n",
    "\texcept:\n",
    "\t\treturn db.query(**parsed)\n",
    "\n",
    "def qquery(**kwargs):\n",
    "    return kwargs\n",
    "\n",
    "def generate_qquery(parsed):\n",
    "\ttry:\n",
    "\t\tif 'subq' in parsed:\n",
    "\t\t\tsubq = parsed['subq']\n",
    "\t\t\tif type(subq) == str:\n",
    "\t\t\t\tparsed['tables'] = [subq]\n",
    "\t\t\t\tdel parsed['subq']\n",
    "\t\t\telse:\n",
    "\t\t\t\tparsed['subq'] = generate_qquery(parsed['subq'])\n",
    "\t\treturn qquery(**parsed)\n",
    "\texcept:\n",
    "\t\treturn qquery(**parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output from the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'agg',\n",
       " 'agg_func': {'function': 'avg', 'field': 'gpa'},\n",
       " 'sorting': 'asc',\n",
       " 'subq': {'method': 'group_by', 'group_by': 'major', 'subq': 'students'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qstring2 = 'find avg(gpa) [asc] by major from students'\n",
    "parsed2 = parser.parse(qstring2)\n",
    "parsed2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`qquery` is a toy function that shows how queries are recursively generated from nested subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'agg',\n",
       " 'agg_func': {'function': 'avg', 'field': 'gpa'},\n",
       " 'sorting': 'asc',\n",
       " 'subq': {'method': 'group_by', 'group_by': 'major', 'tables': ['students']}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_qquery(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which then is passed to the actual `db` to make a real query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Computer Science', 3.395110949963742),\n",
       " ('Data Science', 3.3997494653223344),\n",
       " ('Computer Engineering', 3.401678609544138)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_query(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a more complex query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'agg',\n",
       " 'agg_func': {'function': 'avg', 'field': 'gpa'},\n",
       " 'sorting': None,\n",
       " 'subq': {'method': 'group_by',\n",
       "  'group_by': 'course_name',\n",
       "  'subq': {'method': 'join',\n",
       "   'tables': ['students', 'enrollments', 'courses'],\n",
       "   'on': {'students.student_id': 'enrollments.student_id',\n",
       "    'enrollments.course_id': 'courses.course_id'}}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qstring4 = 'find avg(gpa) by course_name from join students, enrollments, courses on students.student_id: enrollments.student_id, enrollments.course_id: courses.course_id'\n",
    "parsed4 = parser.parse(qstring4)\n",
    "parsed4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'agg',\n",
       " 'agg_func': {'function': 'avg', 'field': 'gpa'},\n",
       " 'sorting': None,\n",
       " 'subq': {'method': 'group_by',\n",
       "  'group_by': 'course_name',\n",
       "  'subq': {'method': 'join',\n",
       "   'tables': ['students', 'enrollments', 'courses'],\n",
       "   'on': {'students.student_id': 'enrollments.student_id',\n",
       "    'enrollments.course_id': 'courses.course_id'}}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_qquery(parsed4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Software Engineering': 3.3916274509803923,\n",
       " 'Introduction to Programming': 3.394664005322688,\n",
       " 'Scientific Computing and Visualization': 3.384058463630184,\n",
       " 'Special Topics': 3.407740039190072,\n",
       " 'Privacy in the World of Big Data': 3.3933125,\n",
       " 'Senior Project': 3.3886577652846097,\n",
       " 'Optimization for the Information and Data Sciences': 3.4040390674228105,\n",
       " 'Introduction to Robotics': 3.3939461588969135,\n",
       " 'Web Technologies': 3.4026281208935614,\n",
       " 'Programming Graphical User Interfaces': 3.391156741957563,\n",
       " 'Search and Planning': 3.40478672985782,\n",
       " 'Geometric Modeling': 3.3932934131736534,\n",
       " 'Explorations in Computing': 3.4056466069142126,\n",
       " 'Introduction to Computer Networks': 3.409338092147956,\n",
       " 'File and Database Management': 3.415058517555267,\n",
       " 'Database Systems Interoperability': 3.392032894736842,\n",
       " 'Digital Geometry Processing': 3.3983025099075297,\n",
       " 'Introduction to Programming Systems Design': 3.402798194713088,\n",
       " 'Game Prototyping': 3.388484848484848,\n",
       " 'Diagnosis and Design of Reliable Digital Systems': 3.4044308943089434,\n",
       " 'Advanced Natural Language Processing': 3.4046693386773548,\n",
       " 'Compiler Development': 3.416864973262032,\n",
       " 'Introduction to Operating Systems': 3.403484549638396,\n",
       " 'Parallel and Distributed Computation': 3.384815546772069,\n",
       " 'Natural Language Dialogue Systems': 3.402207792207792,\n",
       " 'Haptic Interfaces and Virtual Environments': 3.400571428571429,\n",
       " 'Machine Learning Theory': 3.37599582172702,\n",
       " 'Quantum Computing and Quantum Cryptography': 3.3996359890109895,\n",
       " 'Advanced Program Analysis and Verification': 3.3958366013071895,\n",
       " 'Social Media Analytics': 3.39889400921659,\n",
       " 'Video Game Programming': 3.4016701316701314,\n",
       " 'Artificial Intelligence for Sustainable Development': 3.3831297709923667,\n",
       " 'Introduction to Internetworking': 3.4032992501704156,\n",
       " 'Advanced Distributed Systems': 3.3935227272727273,\n",
       " 'Computer Science Research Colloquium': 3.3909053778080325,\n",
       " 'Computer Systems and Applications Modeling Fundamentals': 3.405497113534317,\n",
       " 'Computer Graphics': 3.400879566982409,\n",
       " 'Advanced Information Integration': 3.401489222730242,\n",
       " 'Advanced Topics in Operating Systems': 3.4039584685269304,\n",
       " 'Directed Research': 3.389756179024716,\n",
       " 'Software Engineering for Embedded Systems': 3.407066142763589,\n",
       " 'Fundamentals of Computation': 3.395050778605281,\n",
       " 'High Performance Computing and Simulations': 3.3820202702702704,\n",
       " 'Text as Data': 3.3963682219419926,\n",
       " 'Professional Writing and Communication for Computer Scientists': 3.40644,\n",
       " 'Geospatial Information Management': 3.3984680025856497,\n",
       " 'Computer Systems Organization': 3.387165809768638,\n",
       " 'Data Structures and Object Oriented Design': 3.3904871794871796,\n",
       " 'Advanced Topics in Interconnection Network Design and Analysis': 3.3980149346608592,\n",
       " 'Introduction to Algorithms and Theory of Computing': 3.4089736842105265,\n",
       " 'Numerical Analysis': 3.397066235864297,\n",
       " \"Master's Thesis\": 3.4003141361256546,\n",
       " 'Logic and its Applications': 3.3954641211323238,\n",
       " 'Introduction to Computer Systems': 3.3951523358158426,\n",
       " 'Mathematical Foundations for System Design: Modeling, Analysis, and Synthesis': 3.4017619680851063,\n",
       " 'Concepts of Programming Languages': 3.398465211459754,\n",
       " 'Professional C++': 3.415684348395547,\n",
       " 'Final Game Project': 3.3905147307822547,\n",
       " 'Information Retrieval and Web Search Engines': 3.411405750798722,\n",
       " 'Discrete Methods in Computer Science': 3.410027266530334,\n",
       " 'Computer Organization and Architecture': 3.401048332198775,\n",
       " 'Numerical Solutions of Ordinary and Partial Differential Equations': 3.4013220998055735,\n",
       " 'Multimedia Systems Design': 3.4088287068381855,\n",
       " 'Computer Vision': 3.412108667529108,\n",
       " 'Introduction to Computer and Network Security': 3.401806282722513,\n",
       " 'Native Console Multiplayer Game Development': 3.4044127190136275,\n",
       " '3-D Graphics and Rendering': 3.397658438959306,\n",
       " 'Introduction to Artificial Intelligence': 3.3904524590163936,\n",
       " 'Translation of Programming Languages': 3.3888984168865433,\n",
       " 'Analysis of Algorithms': 3.412958904109589,\n",
       " 'Pipelines for Games and Interactives': 3.404951892238614,\n",
       " 'Introduction to Online Optimization': 3.3944610169491525,\n",
       " 'Capstone: Creating Your High-Tech Startup': 3.400497094899936,\n",
       " 'Operating Systems': 3.4011044176706826,\n",
       " 'Principles of Software Development': 3.407148337595908,\n",
       " 'Program Synthesis and Computer-Aided Verification': 3.411141522029372,\n",
       " 'Introduction to System-on-Chip': 3.396209193870753,\n",
       " 'Theory of Computation': 3.409633577614924,\n",
       " 'Mathematics of High-Dimensional Data': 3.385349444807315,\n",
       " 'Coordinated Mobile Robotics': 3.399974457215837,\n",
       " 'Probabilistic Reasoning': 3.3925634352635003,\n",
       " 'Artificial Intelligence for Social Good': 3.404459907223327,\n",
       " 'Introduction to Computer Science': 3.396340341655716,\n",
       " 'Networked Systems in Cloud Computing': 3.410325644504749,\n",
       " 'Database Systems': 3.3869251517194874,\n",
       " 'Software Architectures': 3.3968754301445285,\n",
       " 'Advanced Topics in Computer System Architecture': 3.398297595841455,\n",
       " 'Computer Animation and Simulation': 3.39418945963976,\n",
       " 'Programming Game Engines': 3.408866666666667,\n",
       " 'Numerical Methods': 3.3896378830083567,\n",
       " 'Internet Measurement': 3.382670262980445,\n",
       " 'Cryptography: Secure Communication and Computation': 3.385424621461488,\n",
       " 'Advanced Computer Networking': 3.4058097165991903,\n",
       " 'Introduction to Machine Learning': 3.398111111111111,\n",
       " 'Numerical Analysis and Computation': 3.393359580052493,\n",
       " 'Capstone: Design and Construction of Large Software Systems': 3.4094391891891895,\n",
       " 'Special Problems': 3.399720186542305,\n",
       " 'Fundamentals of Computer Programming': 3.4051482479784365}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_query(parsed4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
